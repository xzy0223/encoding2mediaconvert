#!/usr/bin/env python3
"""
E2MC Workflow - Complete workflow for Encoding to MediaConvert conversion and comparison

This script provides a complete workflow for:
1. Converting Encoding.com configuration files to AWS MediaConvert configuration files
2. Creating MediaConvert transcoding jobs using the converted configurations
3. Analyzing and comparing the videos generated by both systems

Author: AWS Professional Services
"""

import argparse
import boto3
import json
import logging
import os
import re
import sys
import time
from datetime import datetime
import random
from typing import Dict, Any, List, Optional, Tuple
from pathlib import Path
from botocore.exceptions import ClientError

# Import required modules from the project
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../../../')))
from src.e2mc_assistant.converter.config_converter_enhanced import ConfigConverter
from src.e2mc_assistant.requester.mediaconvert_job_submitter import MediaConvertJobSubmitter
from src.e2mc_assistant.analyzer.video_analyzer import VideoAnalyzer

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class E2MCWorkflow:
    """
    Class to handle the complete workflow from Encoding.com configuration to MediaConvert
    job submission and video comparison.
    """

    # Constants for path handling
    S3_PREFIX = "s3://"

    def __init__(self, region: str = 'us-east-1', role_arn: Optional[str] = None):
        """
        Initialize the workflow handler.

        Args:
            region: AWS region for MediaConvert and S3 operations
            role_arn: IAM role ARN for MediaConvert to access resources
        """
        self.region = region
        self.role_arn = role_arn
        self.s3_client = boto3.client('s3', region_name=region)
        
        # Initialize components
        self.converter = None
        self.job_submitter = None
        self.video_analyzer = None

    def convert_configs(self, input_dir: str, output_dir: str, rules_file: str, template_file: Optional[str] = None, schema_file: Optional[str] = None, include_ids: Optional[List[str]] = None, exclude_ids: Optional[List[str]] = None) -> List[str]:
        """
        Convert Encoding.com configuration files to MediaConvert configuration files.

        Args:
            input_dir: Directory containing Encoding.com configuration files
            output_dir: Directory to save MediaConvert configuration files
            rules_file: Path to the mapping rules file
            template_file: Optional path to a template MediaConvert file
            schema_file: Optional path to a JSON schema file for validation
            include_ids: Optional list of video IDs to include
            exclude_ids: Optional list of video IDs to exclude

        Returns:
            List of paths to the generated MediaConvert configuration files
        """
        # Create output directory if it doesn't exist
        os.makedirs(output_dir, exist_ok=True)
        
        # Initialize converter
        self.converter = ConfigConverter(rules_file)
        
        # Configure logging for converter
        converter_logger = logging.getLogger('ConfigConverter')
        converter_logger.setLevel(logging.INFO)
        
        # Create a file handler for detailed logs
        log_file = os.path.join(output_dir, 'conversion_details.log')
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(logging.DEBUG)
        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
        converter_logger.addHandler(file_handler)
        
        # Track converted files
        converted_files = []
        
        # Process each XML file in the input directory
        for filename in os.listdir(input_dir):
            if filename.endswith('.xml'):
                source_file = os.path.join(input_dir, filename)
                
                # Extract ID from filename (assuming it's a number at the beginning)
                id_match = re.match(r'^(\d+)', filename)
                if id_match:
                    file_id = id_match.group(1)
                else:
                    file_id = os.path.splitext(filename)[0]
                
                # Apply include/exclude filtering
                if include_ids and file_id not in include_ids:
                    logger.info(f"Skipping {filename} - ID {file_id} not in include list")
                    continue
                
                if exclude_ids and file_id in exclude_ids:
                    logger.info(f"Skipping {filename} - ID {file_id} in exclude list")
                    continue
                
                # Define output filename with the same ID prefix
                output_file = os.path.join(output_dir, f"{file_id}.json")
                
                # Create a specific log file for this conversion
                file_log = os.path.join(output_dir, f"{file_id}_conversion.log")
                file_handler = logging.FileHandler(file_log)
                file_handler.setLevel(logging.DEBUG)
                file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
                converter_logger.addHandler(file_handler)
                
                try:
                    # Log the start of conversion
                    converter_logger.info(f"Starting conversion of {source_file}")
                    
                    # Convert the configuration
                    result = self.converter.convert(source_file, template_file)
                    
                    # Write the output file
                    with open(output_file, 'w') as f:
                        json.dump(result, f, indent=2)
                    
                    logger.info(f"Converted {source_file} to {output_file}")
                    converted_files.append(output_file)
                    
                    # Log successful conversion
                    converter_logger.info(f"Successfully converted {source_file} to {output_file}")
                    
                    # Validate the converted file if schema is provided
                    if schema_file:
                        from utils.mc_config_validator.validator import MediaConvertConfigValidator
                        validator = MediaConvertConfigValidator(schema_file)
                        converter_logger.info(f"Validating {output_file} against schema {schema_file}")
                        is_valid = validator.validate_config(output_file)
                        if not is_valid:
                            error_file = os.path.join(output_dir, f"{file_id}.err")
                            
                            # Create a string handler to capture validation errors
                            import io
                            string_io = io.StringIO()
                            string_handler = logging.StreamHandler(string_io)
                            string_handler.setLevel(logging.ERROR)
                            validator.logger.addHandler(string_handler)
                            
                            # Re-run validation to capture errors
                            validator.validate_config(output_file)
                            
                            # Get the captured error messages
                            validator.logger.removeHandler(string_handler)
                            error_messages = string_io.getvalue()
                            
                            # Write detailed error information to the error file
                            with open(error_file, 'w') as f:
                                f.write(f"Validation failed for {output_file}\n")
                                f.write("Validation errors:\n")
                                f.write(error_messages)
                            
                            converter_logger.error(f"Validation failed for {output_file}. Error log written to {error_file}")
                        else:
                            converter_logger.info(f"Validation successful for {output_file}")
                    
                except Exception as e:
                    logger.error(f"Error converting {source_file}: {str(e)}")
                    converter_logger.error(f"Error converting {source_file}: {str(e)}")
                
                # Remove the file-specific handler
                converter_logger.removeHandler(file_handler)
                file_handler.close()
        
        logger.info(f"Converted {len(converted_files)} configuration files")
        print(f"Detailed conversion logs saved to {log_file} and individual files in {output_dir}")
        return converted_files

    def submit_mediaconvert_jobs(self, config_dir: str, s3_source_path: str, wait_for_completion: bool = True, include_ids: Optional[List[str]] = None, exclude_ids: Optional[List[str]] = None, s3_output_path: Optional[str] = None) -> Dict[str, str]:
        """
        Submit MediaConvert jobs for each configuration file.

        Args:
            config_dir: Directory containing MediaConvert configuration files
            s3_source_path: S3 path where source videos are stored
            wait_for_completion: Whether to wait for each job to complete before submitting the next
            include_ids: Optional list of IDs to include (only these IDs will be processed)
            exclude_ids: Optional list of IDs to exclude from processing
            s3_output_path: Optional S3 path for MediaConvert output files

        Returns:
            Dictionary mapping job IDs to their status
        """
        # Initialize job submitter
        self.job_submitter = MediaConvertJobSubmitter(
            region=self.region,
            role_arn=self.role_arn
        )
        
        # Track job IDs and status
        job_results = {}
        
        # Process each JSON file in the config directory
        for filename in sorted(os.listdir(config_dir)):
            if filename.endswith('.json'):
                # Extract ID from filename (assuming it's a number at the beginning)
                id_match = re.match(r'^(\d+)', filename)
                if id_match:
                    file_id = id_match.group(1)
                else:
                    file_id = os.path.splitext(filename)[0]
                
                # Check if this ID should be included/excluded
                if include_ids and file_id not in include_ids:
                    continue
                if exclude_ids and file_id in exclude_ids:
                    continue
                
                config_file = os.path.join(config_dir, filename)
                
                # Create a job log file in the same directory as the config file
                job_log_file = os.path.join(config_dir, f"{file_id}_job_submission.log")
                
                try:
                    # Load job profile
                    job_profile = self.job_submitter.load_job_profile(config_file)
                    
                    # Find source video
                    source_video = self._find_source_video(s3_source_path, file_id)
                    if not source_video:
                        logger.warning(f"No source video found for ID {file_id}, skipping")
                        continue
                    
                    # Ensure Inputs section exists and has FileInput
                    if not job_profile['Settings'].get('Inputs'):
                        job_profile['Settings']['Inputs'] = [{}]
                    if not job_profile['Settings']['Inputs'][0]:
                        job_profile['Settings']['Inputs'][0] = {}
                    
                    # Update input URL and output destination
                    job_profile['Settings']['Inputs'][0]['FileInput'] = source_video
                    if s3_output_path:
                        # Use custom output path
                        output_destination = f"{s3_output_path.rstrip('/')}/{file_id}/"
                    else:
                        # Use default pattern
                        output_destination = f"{s3_source_path.rstrip('/')}/{file_id}/"
                    job_profile = self.job_submitter.update_output_destination(job_profile, output_destination)
                    
                    # Submit the job
                    response = self.job_submitter.submit_job(job_profile)
                    job_id = response['Job']['Id']
                    logger.info(f"Submitted job for {file_id} with job ID: {job_id}")
                    
                    # Wait for job completion if requested
                    if wait_for_completion:
                        logger.info(f"Waiting for job {job_id} to complete...")
                        job = self.job_submitter.track_job(job_id)
                        job_results[f"{file_id}:{job_id}"] = job['Status']  # Store with file_id prefix
                        
                        # Log job completion status
                        if job['Status'] != MediaConvertJobSubmitter.STATUS_COMPLETE:
                            logger.warning(f"Job {job_id} completed with status: {job['Status']}")
                            
                            # Create error log file for failed jobs
                            if job['Status'] == MediaConvertJobSubmitter.STATUS_ERROR:
                                error_file = os.path.join(config_dir, f"{file_id}_job_execution.err")
                                with open(error_file, 'w') as f:
                                    f.write(f"MediaConvert job {job_id} failed\n")
                                    f.write(f"Timestamp: {datetime.now().isoformat()}\n\n")
                                    f.write("Job details:\n")
                                    try:
                                        f.write(json.dumps(job, indent=2, default=str))
                                    except Exception as json_err:
                                        f.write(f"Could not serialize job details: {str(json_err)}")
                                    
                                    # Add error messages if available
                                    if 'ErrorMessage' in job:
                                        f.write("\n\nError message:\n")
                                        f.write(job['ErrorMessage'])
                                    elif 'ErrorCode' in job:
                                        f.write("\n\nError code:\n")
                                        f.write(job['ErrorCode'])
                                        
                                logger.error(f"Job {job_id} failed. Error details written to {error_file}")
                        else:
                            # Job completed successfully, upload JSON file with timestamp suffix
                            try:
                                # Parse S3 path - use s3_output_path if provided, otherwise use s3_source_path
                                upload_path = s3_output_path if s3_output_path else s3_source_path
                                bucket_name, prefix = self._parse_s3_path(upload_path)
                                
                                # Generate timestamp suffix for the JSON file
                                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                                json_filename = f"{file_id}_{timestamp}.json"
                                json_s3_key = f"{prefix}/{file_id}/{json_filename}"
                                
                                # Read the current JSON file content
                                with open(config_file, 'r') as f:
                                    current_json_content = f.read()
                                
                                # Check if there are previous JSON files in S3
                                previous_json = self._find_previous_json(bucket_name, f"{prefix}/{file_id}")
                                
                                # Initialize update log content
                                update_log_content = f"Update log for {file_id} - {timestamp}\n"
                                update_log_content += f"Job ID: {job_id}\n"
                                update_log_content += f"Status: {job['Status']}\n\n"
                                
                                # Compare with previous JSON if it exists
                                if previous_json:
                                    logger.info(f"Found previous JSON file: {previous_json}")
                                    update_log_content += f"Comparing with previous JSON: {previous_json}\n\n"
                                    
                                    # Download previous JSON content
                                    previous_json_key = previous_json.replace(f"s3://{bucket_name}/", "")
                                    previous_json_content = self._get_s3_content(bucket_name, previous_json_key)
                                    
                                    # Compare JSON content
                                    try:
                                        import difflib
                                        previous_json_obj = json.loads(previous_json_content)
                                        current_json_obj = json.loads(current_json_content)
                                        
                                        # Convert to formatted JSON strings for better diff
                                        previous_formatted = json.dumps(previous_json_obj, indent=2, sort_keys=True)
                                        current_formatted = json.dumps(current_json_obj, indent=2, sort_keys=True)
                                        
                                        # Generate diff
                                        diff = difflib.unified_diff(
                                            previous_formatted.splitlines(),
                                            current_formatted.splitlines(),
                                            fromfile=previous_json,
                                            tofile=json_filename,
                                            lineterm=''
                                        )
                                        
                                        # Add diff to update log
                                        diff_content = '\n'.join(list(diff))
                                        if diff_content:
                                            update_log_content += "Changes found:\n"
                                            update_log_content += diff_content
                                            update_log_content += "\n\n"
                                        else:
                                            update_log_content += "No changes detected between JSON files.\n\n"
                                    except Exception as e:
                                        update_log_content += f"Error comparing JSON files: {str(e)}\n\n"
                                else:
                                    update_log_content += "No previous JSON file found. This is the first version.\n\n"
                                
                                # Upload the JSON file to S3
                                self.s3_client.put_object(
                                    Bucket=bucket_name,
                                    Key=json_s3_key,
                                    Body=current_json_content
                                )
                                logger.info(f"Uploaded JSON file to s3://{bucket_name}/{json_s3_key}")
                                
                                # Update the update.log file in the video ID's prefix directory
                                update_log_key = f"{prefix}/{file_id}/update.log"
                                
                                # Check if update.log already exists
                                try:
                                    existing_log = self._get_s3_content(bucket_name, update_log_key)
                                    update_log_content = existing_log + "\n" + update_log_content
                                except Exception:
                                    # Log doesn't exist yet, use the new content
                                    pass
                                
                                # Upload the update log to S3
                                self.s3_client.put_object(
                                    Bucket=bucket_name,
                                    Key=update_log_key,
                                    Body=update_log_content
                                )
                                logger.info(f"Updated log file at s3://{bucket_name}/{update_log_key}")
                                
                            except Exception as e:
                                logger.error(f"Error handling JSON upload for {file_id}: {str(e)}")
                    else:
                        job_results[f"{file_id}:{job_id}"] = "SUBMITTED"  # Store with file_id prefix
                    
                except Exception as e:
                    error_msg = f"Error submitting job for {file_id}: {str(e)}"
                    logger.error(error_msg)
                    job_results[file_id] = f"ERROR: {str(e)}"
                    
                    # Create error log file for submission errors
                    error_file = os.path.join(config_dir, f"{file_id}_job_submission.err")
                    with open(error_file, 'w') as f:
                        f.write(f"Error submitting MediaConvert job for {file_id}\n")
                        f.write(f"Timestamp: {datetime.now().isoformat()}\n\n")
                        f.write(f"Error message: {str(e)}\n\n")
                        f.write("Job settings:\n")
                        try:
                            f.write(json.dumps(job_profile, indent=2, default=str))
                        except Exception as json_err:
                            f.write(f"Could not serialize job profile: {str(json_err)}")
                    
                    logger.error(f"Job submission error details written to {error_file}")
        
        return job_results

    def _find_source_video(self, s3_path: str, file_id: str) -> Optional[str]:
        """
        Find a source video in S3 with the given file ID prefix.

        Args:
            s3_path: S3 path where source videos are stored
            file_id: ID prefix to search for

        Returns:
            S3 URL of the source video if found, None otherwise
        """
        # Parse S3 path
        bucket_name, prefix = self._parse_s3_path(s3_path)
        
        try:
            # Construct the full prefix for the specific ID
            id_prefix = f"{prefix}/{file_id}/" if prefix else f"{file_id}/"
            
            # List objects in the specific ID directory
            response = self.s3_client.list_objects_v2(
                Bucket=bucket_name,
                Prefix=id_prefix
            )
            
            if not response.get('Contents'):
                logger.warning(f"No files found in {s3_path}/{file_id}")
                return None
            
            # Look for source video files with pattern: {id}_*_source.{suffix}
            # Explicitly exclude files containing "_output" or "_mc" in their names
            video_extensions = ['.mp4', '.mov', '.mpg', '.mpeg', '.mxf', '.webm']
            source_files = []
            
            for obj in response.get('Contents', []):
                key = obj['Key']
                filename = os.path.basename(key)
                ext = os.path.splitext(filename)[1].lower()
                
                # Skip files with _mc or _output in their names
                if "_mc" in filename or "_output" in filename:
                    continue
                
                # First priority: files with _source in their name
                if "_source." in filename and ext in video_extensions:
                    source_files.append(key)
                    continue
                
                # Second priority: any video file that matches the ID pattern
                if ext in video_extensions and filename.startswith(f"{file_id}_"):
                    source_files.append(key)
            
            if source_files:
                # Sort files to ensure consistent selection
                source_files.sort()
                selected_file = source_files[0]
                logger.debug(f"Selected source file: {selected_file}")
                return f"s3://{bucket_name}/{selected_file}"
            
            logger.warning(f"No suitable source video found for ID {file_id}")
            return None
            
        except Exception as e:
            logger.error(f"Error searching for source video: {str(e)}")
            return None

    def _parse_s3_path(self, s3_path: str) -> Tuple[str, str]:
        """
        Parse an S3 path into bucket name and prefix.

        Args:
            s3_path: S3 path (s3://bucket-name/prefix)

        Returns:
            Tuple of (bucket_name, prefix)
        """
        # Remove s3:// prefix if present
        if s3_path.startswith('s3://'):
            s3_path = s3_path[5:]
        
        # Split into bucket and prefix
        parts = s3_path.split('/', 1)
        bucket = parts[0]
        prefix = parts[1] if len(parts) > 1 else ""
        
        # Remove trailing slash from prefix
        if prefix and prefix.endswith('/'):
            prefix = prefix[:-1]
        
        return bucket, prefix

    def analyze_videos(self, s3_path: str, include_ids: Optional[List[str]] = None, exclude_ids: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        Analyze and compare videos in the given S3 path.

        Args:
            s3_path: S3 path containing videos to analyze
            include_ids: Optional list of video IDs to include (only these IDs will be analyzed)
            exclude_ids: Optional list of video IDs to exclude from analysis

        Returns:
            Dictionary containing analysis results
        """
        # Initialize video analyzer
        self.video_analyzer = VideoAnalyzer(region=self.region)
        
        # Parse S3 path
        bucket_name, prefix = self._parse_s3_path(s3_path)
        
        # Get list of all subdirectories (video IDs)
        video_ids = self._list_video_ids(bucket_name, prefix)
        if not video_ids:
            logger.warning(f"No video IDs found in {s3_path}")
            return {}
            
        # Filter video IDs based on include/exclude lists
        if include_ids:
            video_ids = [vid for vid in video_ids if vid in include_ids]
            logger.info(f"Filtered to include only IDs: {video_ids}")
        if exclude_ids:
            video_ids = [vid for vid in video_ids if vid not in exclude_ids]
            logger.info(f"Excluded IDs: {exclude_ids}")
            
        results = {}
        for video_id in video_ids:
            logger.info(f"Starting analysis for video ID: {video_id}")
            
            # Find target videos (original and MediaConvert)
            original_file = self._find_target_video(bucket_name, f"{prefix}/{video_id}")
            mc_file = self._find_mc_video(bucket_name, f"{prefix}/{video_id}")
            
            if not original_file or not mc_file:
                logger.warning(f"Missing files for video ID {video_id}")
                logger.warning(f"Original file: {original_file}")
                logger.warning(f"MC file: {mc_file}")
                self._save_analysis_summary(video_id, "FAILED", "Missing files")
                continue
            
            try:
                # Check file extension to determine analysis method
                original_ext = os.path.splitext(original_file)[1].lower()
                mc_ext = os.path.splitext(mc_file)[1].lower()
                
                # For HLS manifests (.m3u8), use ffmpeg analysis
                if original_ext == '.m3u8' and mc_ext == '.m3u8':
                    logger.info(f"Analyzing HLS streams for video ID {video_id}")
                    original_info = self.video_analyzer.extract_video_info(original_file)
                    mc_info = self.video_analyzer.extract_video_info(mc_file)
                    
                    # Generate comprehensive report
                    report = self.video_analyzer.generate_report(
                        original_info=original_info,
                        mc_info=mc_info,
                        s3_client=self.s3_client,
                        bucket_name=bucket_name,
                        report_prefix=f"{prefix}/{video_id}/reports"
                    )
                    
                    # Add to results
                    results[video_id] = {
                        'original_video': original_file,
                        'mc_video': mc_file,
                        'has_differences': bool(report.get('differences')),
                        'report_paths': report.get('report_paths', {})
                    }
                    
                    # Save analysis summary
                    self._save_analysis_summary(video_id, "SUCCESS", "HLS analysis completed", 
                                             has_differences=bool(report.get('differences')))
                
                # For DASH manifests (.mpd), compare manifest content
                elif original_ext == '.mpd' and mc_ext == '.mpd':
                    logger.info(f"Analyzing DASH manifests for video ID {video_id}")
                    
                    # Download manifests from S3
                    original_content = self._get_s3_content(bucket_name, original_file.replace(f"s3://{bucket_name}/", ""))
                    mc_content = self._get_s3_content(bucket_name, mc_file.replace(f"s3://{bucket_name}/", ""))
                    
                    # Create report directory
                    report_prefix = f"{prefix}/{video_id}/reports"
                    
                    # Save manifests and create a simple comparison report
                    report = {
                        'original_manifest': original_content,
                        'mc_manifest': mc_content,
                        'has_differences': original_content != mc_content
                    }
                    
                    # Save report to S3
                    report_key = f"{report_prefix}/manifest_comparison.json"
                    self._save_to_s3(bucket_name, report_key, json.dumps(report, indent=2))
                    
                    # Add to results
                    results[video_id] = {
                        'original_manifest': original_file,
                        'mc_manifest': mc_file,
                        'has_differences': report['has_differences'],
                        'report_paths': {
                            'manifest_comparison': f"s3://{bucket_name}/{report_key}"
                        }
                    }
                    
                    # Save analysis summary
                    self._save_analysis_summary(video_id, "SUCCESS", "DASH analysis completed", 
                                             has_differences=report['has_differences'])
                
                # For MP4 files, use ffmpeg analysis
                elif original_ext == '.mp4' and mc_ext == '.mp4':
                    logger.info(f"Analyzing MP4 files for video ID {video_id}")
                    original_info = self.video_analyzer.extract_video_info(original_file)
                    mc_info = self.video_analyzer.extract_video_info(mc_file)
                    
                    # Generate comprehensive report
                    report = self.video_analyzer.generate_report(
                        original_info=original_info,
                        mc_info=mc_info,
                        s3_client=self.s3_client,
                        bucket_name=bucket_name,
                        report_prefix=f"{prefix}/{video_id}/reports"
                    )
                    
                    # Add to results
                    results[video_id] = {
                        'original_video': original_file,
                        'mc_video': mc_file,
                        'has_differences': bool(report.get('differences')),
                        'report_paths': report.get('report_paths', {})
                    }
                    
                    # Save analysis summary
                    self._save_analysis_summary(video_id, "SUCCESS", "MP4 analysis completed", 
                                             has_differences=bool(report.get('differences')))
                
                else:
                    error_msg = f"Unsupported file types: original={original_ext}, mc={mc_ext}"
                    logger.error(error_msg)
                    self._save_analysis_summary(video_id, "FAILED", error_msg)
                    results[video_id] = {
                        'error': error_msg
                    }
                
                logger.info(f"Completed analysis for video ID {video_id}")
                
            except Exception as e:
                error_msg = f"Error analyzing files: {str(e)}"
                logger.error(f"Error analyzing video ID {video_id}: {error_msg}")
                self._save_analysis_summary(video_id, "FAILED", error_msg)
                results[video_id] = {
                    'original_file': original_file,
                    'mc_file': mc_file,
                    'error': error_msg
                }
        
        return results

    def _save_analysis_summary(self, video_id: str, status: str, message: str, has_differences: bool = None):
        """
        Save analysis summary to a local file.

        Args:
            video_id: Video ID
            status: Analysis status (SUCCESS/FAILED)
            message: Status message
            has_differences: Whether differences were found (optional)
        """
        summary_file = "analysis_summary.json"
        
        # Load existing summary if it exists
        if os.path.exists(summary_file):
            with open(summary_file, 'r') as f:
                summary = json.load(f)
        else:
            summary = {'analyzed_videos': []}
        
        # Add new entry
        entry = {
            'video_id': video_id,
            'status': status,
            'message': message,
            'timestamp': datetime.now().isoformat()
        }
        if has_differences is not None:
            entry['has_differences'] = has_differences
        
        # Update or add entry
        updated = False
        for i, item in enumerate(summary['analyzed_videos']):
            if item['video_id'] == video_id:
                summary['analyzed_videos'][i] = entry
                updated = True
                break
        
        if not updated:
            summary['analyzed_videos'].append(entry)
        
        # Save summary
        with open(summary_file, 'w') as f:
            json.dump(summary, f, indent=2)
            
    def _get_s3_content(self, bucket_name: str, key: str) -> str:
        """
        Get content of an S3 object as string.

        Args:
            bucket_name: S3 bucket name
            key: S3 object key

        Returns:
            Content of the S3 object as string
        """
        try:
            response = self.s3_client.get_object(Bucket=bucket_name, Key=key)
            return response['Body'].read().decode('utf-8')
        except Exception as e:
            logger.error(f"Error reading S3 object {key}: {str(e)}")
            return ""

    def _list_video_ids(self, bucket_name: str, prefix: str) -> List[str]:
        """
        List all video IDs (subdirectories) in the given S3 path.

        Args:
            bucket_name: S3 bucket name
            prefix: S3 prefix to list

        Returns:
            List of video IDs
        """
        video_ids = []
        
        try:
            # Ensure prefix ends with a slash
            if prefix and not prefix.endswith('/'):
                prefix = f"{prefix}/"
            
            # Use delimiter to list "directories"
            paginator = self.s3_client.get_paginator('list_objects_v2')
            for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix, Delimiter='/'):
                for prefix_obj in page.get('CommonPrefixes', []):
                    # Extract the ID from the prefix
                    prefix_path = prefix_obj['Prefix']
                    if prefix_path.startswith(prefix):
                        prefix_path = prefix_path[len(prefix):]
                    video_id = prefix_path.rstrip('/').split('/')[0]
                    if video_id.isdigit():  # Only include numeric IDs
                        video_ids.append(video_id)
            
            return sorted(list(set(video_ids)))  # Remove duplicates and sort
            
        except Exception as e:
            logger.error(f"Error listing video IDs: {str(e)}")
            return []



    def _find_target_video(self, bucket_name: str, prefix: str) -> Optional[str]:
        """
        Find the target video in S3 with pattern *_target.{suffix}

        Args:
            bucket_name: S3 bucket name
            prefix: S3 prefix to search in

        Returns:
            S3 URL of the video if found, None otherwise
        """
        try:
            # List objects with the prefix
            response = self.s3_client.list_objects_v2(
                Bucket=bucket_name,
                Prefix=prefix
            )
            
            if not response.get('Contents'):
                logger.warning(f"No files found in {prefix}")
                return None
            
            # Look for files matching the pattern *_target.{suffix}
            matching_files = []
            for obj in response.get('Contents', []):
                key = obj['Key']
                filename = os.path.basename(key)
                if "_target." in filename and not any(x in filename for x in ["_mc", "_output"]):
                    matching_files.append(key)
            
            # If we found exactly one matching file, use it
            if len(matching_files) == 1:
                key = matching_files[0]
                logger.debug(f"Found unique target video: {key}")
                return f"s3://{bucket_name}/{key}"
            
            # If we found multiple matching files, use the first one
            elif len(matching_files) > 1:
                key = matching_files[0]
                logger.debug(f"Using first target video: {key}")
                return f"s3://{bucket_name}/{key}"
            
            # If we found no matching files, log a warning
            else:
                logger.warning(f"No target video found in {prefix}")
                return None
                
        except Exception as e:
            logger.error(f"Error searching for target video: {str(e)}")
            return None

    def _find_mc_video(self, bucket_name: str, prefix: str) -> Optional[str]:
        """
        Find the MediaConvert video in S3 with pattern *_mc.{suffix}

        Args:
            bucket_name: S3 bucket name
            prefix: S3 prefix to search in

        Returns:
            S3 URL of the video if found, None otherwise
        """
        try:
            # List objects with the prefix
            response = self.s3_client.list_objects_v2(
                Bucket=bucket_name,
                Prefix=prefix
            )
            
            if not response.get('Contents'):
                logger.warning(f"No files found in {prefix}")
                return None
            
            # Look for files matching the pattern *_mc.{suffix}
            matching_files = []
            for obj in response.get('Contents', []):
                key = obj['Key']
                filename = os.path.basename(key)
                if "_mc." in filename and not any(x in filename for x in ["_1280", "_320", "_512", "_640", "_768", "_audio"]):
                    matching_files.append(key)
            
            # If we found exactly one matching file, use it
            if len(matching_files) == 1:
                key = matching_files[0]
                logger.debug(f"Found unique MC video: {key}")
                return f"s3://{bucket_name}/{key}"
            
            # If we found multiple matching files, use the first one
            elif len(matching_files) > 1:
                key = matching_files[0]
                logger.debug(f"Using first MC video: {key}")
                return f"s3://{bucket_name}/{key}"
            
            # If we found no matching files, log a warning
            else:
                logger.warning(f"No MediaConvert video found in {prefix}")
                return None
                
        except Exception as e:
            logger.error(f"Error searching for MediaConvert video: {str(e)}")
            return None
            
    def _find_previous_json(self, bucket_name: str, prefix: str) -> Optional[str]:
        """
        Find the most recent JSON file in the given S3 prefix.

        Args:
            bucket_name: S3 bucket name
            prefix: S3 prefix to search in

        Returns:
            S3 URL of the most recent JSON file if found, None otherwise
        """
        try:
            # List objects with the prefix
            response = self.s3_client.list_objects_v2(
                Bucket=bucket_name,
                Prefix=prefix
            )
            
            if not response.get('Contents'):
                logger.debug(f"No files found in {prefix}")
                return None
            
            # Look for JSON files
            json_files = []
            for obj in response.get('Contents', []):
                key = obj['Key']
                if key.endswith('.json'):
                    json_files.append({
                        'key': key,
                        'last_modified': obj['LastModified']
                    })
            
            # If we found JSON files, return the most recent one
            if json_files:
                # Sort by last modified date (newest first)
                json_files.sort(key=lambda x: x['last_modified'], reverse=True)
                most_recent = json_files[0]['key']
                logger.debug(f"Found most recent JSON file: {most_recent}")
                return f"s3://{bucket_name}/{most_recent}"
            
            logger.debug(f"No JSON files found in {prefix}")
            return None
                
        except Exception as e:
            logger.error(f"Error searching for JSON files: {str(e)}")
            return None
            
    def _find_mc_video(self, bucket_name: str, prefix: str) -> Optional[str]:
        """
        Find the MediaConvert video in S3 with pattern {id}_{可忽略的字符串}_source_{可忽略的字符串}_{可忽略的字符串}_mc.{suffix}

        Args:
            bucket_name: S3 bucket name
            prefix: S3 prefix to search in

        Returns:
            S3 URL of the video if found, None otherwise
        """
        try:
            # List objects with the prefix
            response = self.s3_client.list_objects_v2(
                Bucket=bucket_name,
                Prefix=prefix
            )
            
            # Extract ID from prefix for matching
            prefix_parts = prefix.rstrip('/').split('/')
            file_id = prefix_parts[-1]
            
            # Look for files matching the pattern {id}_*_mc.{suffix}
            import re
            pattern = re.compile(f"^{re.escape(prefix)}{re.escape(file_id)}_.*_mc\\.[a-zA-Z0-9]+$", re.IGNORECASE)
            
            for obj in response.get('Contents', []):
                key = obj['Key']
                if pattern.match(key):
                    logger.debug(f"Found MC video: {key}")
                    return f"s3://{bucket_name}/{key}"
            
            # Fallback to simpler pattern if regex didn't match
            for obj in response.get('Contents', []):
                key = obj['Key']
                if "_source_" in key.lower() and "_mc." in key.lower():
                    logger.debug(f"Found MC video (fallback): {key}")
                    return f"s3://{bucket_name}/{key}"
            
            logger.warning(f"No MediaConvert video found in {prefix}")
            return None
        except Exception as e:
            logger.error(f"Error searching for MediaConvert video: {str(e)}")
            return None




    def _save_to_s3(self, bucket_name: str, key: str, content: str) -> bool:
        """
        Save content to an S3 object.

        Args:
            bucket_name: S3 bucket name
            key: S3 object key
            content: Content to save

        Returns:
            True if successful, False otherwise
        """
        try:
            self.s3_client.put_object(
                Bucket=bucket_name,
                Key=key,
                Body=content
            )
            return True
        except Exception as e:
            logger.error(f"Error saving to S3: {str(e)}")
            return False


def parse_arguments():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description='E2MC Workflow - Complete workflow for Encoding to MediaConvert conversion and comparison'
    )
    
    # Create subparsers for different commands
    subparsers = parser.add_subparsers(dest='command', help='Command to execute')
    
    # Convert command
    convert_parser = subparsers.add_parser(
        'convert',
        help='Convert Encoding.com configuration files to MediaConvert configuration files'
    )
    convert_parser.add_argument(
        '--input-dir',
        required=True,
        help='Directory containing Encoding.com configuration files'
    )
    convert_parser.add_argument(
        '--output-dir',
        required=True,
        help='Directory to save MediaConvert configuration files'
    )
    convert_parser.add_argument(
        '--rules-file',
        required=True,
        help='Path to the mapping rules file'
    )
    convert_parser.add_argument(
        '--template-file',
        help='Optional path to a template MediaConvert file'
    )
    convert_parser.add_argument(
        '--region',
        default='us-east-1',
        help='AWS region (default: us-east-1)'
    )
    convert_parser.add_argument(
        '--validate',
        help='Path to JSON schema file for validation'
    )
    convert_parser.add_argument(
        '--include',
        help='Comma-separated list of video IDs to include'
    )
    convert_parser.add_argument(
        '--exclude',
        help='Comma-separated list of video IDs to exclude'
    )
    
    # Submit command
    submit_parser = subparsers.add_parser(
        'submit',
        help='Submit MediaConvert jobs for configuration files'
    )
    submit_parser.add_argument(
        '--config-dir',
        required=True,
        help='Directory containing MediaConvert configuration files'
    )
    submit_parser.add_argument(
        '--s3-source-path',
        required=True,
        help='S3 path where source videos are stored (s3://bucket-name/prefix)'
    )
    submit_parser.add_argument(
        '--region',
        default='us-east-1',
        help='AWS region (default: us-east-1)'
    )
    submit_parser.add_argument(
        '--role-arn',
        help='IAM role ARN for MediaConvert to access resources'
    )
    submit_parser.add_argument(
        '--no-wait',
        action='store_true',
        help='Do not wait for jobs to complete before submitting the next'
    )
    submit_parser.add_argument(
        '--include',
        help='Comma-separated list of video IDs to include'
    )
    submit_parser.add_argument(
        '--exclude',
        help='Comma-separated list of video IDs to exclude'
    )
    submit_parser.add_argument(
        '--s3-output-path',
        help='S3 path for MediaConvert output files (overrides default s3-source-path/{id}/ pattern)'
    )
    
    # Analyze command
    analyze_parser = subparsers.add_parser(
        'analyze',
        help='Analyze and compare videos'
    )
    analyze_parser.add_argument(
        '--s3-path',
        required=True,
        help='S3 path containing videos to analyze (s3://bucket-name/prefix)'
    )
    analyze_parser.add_argument(
        '--region',
        default='us-east-1',
        help='AWS region (default: us-east-1)'
    )
    analyze_parser.add_argument(
        '--use-llm',
        action='store_true',
        help='Use LLM (Amazon Bedrock) to analyze video differences'
    )
    analyze_parser.add_argument(
        '--include',
        help='Comma-separated list of video IDs to include'
    )
    analyze_parser.add_argument(
        '--exclude',
        help='Comma-separated list of video IDs to exclude'
    )
    
    # Full workflow command
    workflow_parser = subparsers.add_parser(
        'workflow',
        help='Run the complete workflow (convert, submit, analyze)'
    )
    workflow_parser.add_argument(
        '--input-dir',
        required=True,
        help='Directory containing Encoding.com configuration files'
    )
    workflow_parser.add_argument(
        '--output-dir',
        required=True,
        help='Directory to save MediaConvert configuration files'
    )
    workflow_parser.add_argument(
        '--rules-file',
        required=True,
        help='Path to the mapping rules file'
    )
    workflow_parser.add_argument(
        '--template-file',
        help='Optional path to a template MediaConvert file'
    )
    workflow_parser.add_argument(
        '--s3-source-path',
        required=True,
        help='S3 path where source videos are stored (s3://bucket-name/prefix)'
    )
    workflow_parser.add_argument(
        '--region',
        default='us-east-1',
        help='AWS region (default: us-east-1)'
    )
    workflow_parser.add_argument(
        '--role-arn',
        help='IAM role ARN for MediaConvert to access resources'
    )
    workflow_parser.add_argument(
        '--no-wait',
        action='store_true',
        help='Do not wait for jobs to complete before submitting the next'
    )
    workflow_parser.add_argument(
        '--include',
        help='Comma-separated list of video IDs to include'
    )
    workflow_parser.add_argument(
        '--exclude',
        help='Comma-separated list of video IDs to exclude'
    )
    
    return parser.parse_args()


def main():
    """Main function."""
    args = parse_arguments()
    
    if not args.command:
        print("No command specified. Use --help for usage information.")
        return 1
    
    try:
        # Initialize workflow handler
        workflow = E2MCWorkflow(
            region=args.region,
            role_arn=getattr(args, 'role_arn', None)
        )
        
        if args.command == 'convert':
            # 处理 include 和 exclude 参数
            include_ids = args.include.split(',') if args.include else None
            exclude_ids = args.exclude.split(',') if args.exclude else None
            
            if include_ids:
                print(f"Including only IDs: {include_ids}")
            if exclude_ids:
                print(f"Excluding IDs: {exclude_ids}")
            
            # Convert configuration files
            converted_files = workflow.convert_configs(
                input_dir=args.input_dir,
                output_dir=args.output_dir,
                rules_file=args.rules_file,
                template_file=args.template_file,
                schema_file=args.validate if hasattr(args, 'validate') else None,
                include_ids=include_ids,
                exclude_ids=exclude_ids
            )
            
            print(f"Converted {len(converted_files)} configuration files")
            return 0
            
        elif args.command == 'submit':
            # 处理 include 和 exclude 参数
            include_ids = args.include.split(',') if args.include else None
            exclude_ids = args.exclude.split(',') if args.exclude else None
            
            if include_ids:
                print(f"Including only IDs: {include_ids}")
            if exclude_ids:
                print(f"Excluding IDs: {exclude_ids}")
            
            # Submit MediaConvert jobs
            job_results = workflow.submit_mediaconvert_jobs(
                config_dir=args.config_dir,
                s3_source_path=args.s3_source_path,
                wait_for_completion=not args.no_wait,
                include_ids=include_ids,
                exclude_ids=exclude_ids,
                s3_output_path=getattr(args, 's3_output_path', None)
            )
            
            # Create summary log file path
            summary_log_file = os.path.join(args.config_dir, "job_execution_summary.log")
            
            # Get current timestamp
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            
            # Create summary content
            summary_content = f"\n--- Job Execution Summary ({timestamp}) ---\n"
            summary_content += f"Submitted {len(job_results)} MediaConvert jobs\n"
            
            # Print summary to console
            print(f"Submitted {len(job_results)} MediaConvert jobs")
            
            # Track success and failure counts
            success_count = 0
            error_count = 0
            
            for job_id, result in job_results.items():
                # Extract file_id if available (it's in the format {file_id}: ERROR: {message})
                file_id = None
                if ":" in job_id and not job_id.startswith("ERROR"):
                    file_id = job_id
                    job_status = result
                    if job_status == MediaConvertJobSubmitter.STATUS_COMPLETE:
                        success_count += 1
                    else:
                        error_count += 1
                    # Print to console with file ID
                    print(f"File ID: {file_id}, Status: {job_status}")
                    # Add to summary
                    summary_content += f"File ID: {file_id}, Status: {job_status}\n"
                else:
                    # This is a regular job ID
                    job_status = result
                    if job_status == MediaConvertJobSubmitter.STATUS_COMPLETE:
                        success_count += 1
                    else:
                        error_count += 1
                    # Print to console
                    print(f"Job {job_id}: {job_status}")
                    # Add to summary
                    summary_content += f"Job {job_id}: {job_status}\n"
            
            # Add success/error counts to summary
            summary_content += f"Success: {success_count}, Errors: {error_count}\n"
            summary_content += "--- End of Summary ---\n"
            
            # Append to summary log file
            with open(summary_log_file, 'a') as f:
                f.write(summary_content)
            
            print(f"Summary log appended to {summary_log_file}")
            
            return 0
            
        elif args.command == 'analyze':
            # Process include and exclude parameters
            include_ids = args.include.split(',') if args.include else None
            exclude_ids = args.exclude.split(',') if args.exclude else None
            
            if include_ids:
                print(f"Including only IDs: {include_ids}")
            if exclude_ids:
                print(f"Excluding IDs: {exclude_ids}")
            
            # Analyze videos
            analysis_results = workflow.analyze_videos(
                s3_path=args.s3_path,
                include_ids=include_ids,
                exclude_ids=exclude_ids
            )
            
            print(f"Analyzed {len(analysis_results)} video pairs")
            for file_id, result in analysis_results.items():
                if 'error' in result:
                    print(f"ID {file_id}: Error - {result['error']}")
                else:
                    diff_status = "Has differences" if result['has_differences'] else "No differences"
                    print(f"ID {file_id}: {diff_status}")
                    print(f"  Reports saved to:")
                    for report_type, path in result['report_paths'].items():
                        if path:
                            print(f"    - {report_type}: {path}")
            
            return 0
            
        elif args.command == 'workflow':
            # Run the complete workflow
            
            # 处理 include 和 exclude 参数
            include_ids = args.include.split(',') if args.include else None
            exclude_ids = args.exclude.split(',') if args.exclude else None
            
            if include_ids:
                print(f"Including only IDs: {include_ids}")
            if exclude_ids:
                print(f"Excluding IDs: {exclude_ids}")
            
            # Step 1: Convert configuration files
            print("Step 1: Converting configuration files...")
            converted_files = workflow.convert_configs(
                input_dir=args.input_dir,
                output_dir=args.output_dir,
                rules_file=args.rules_file,
                template_file=args.template_file
            )
            print(f"Converted {len(converted_files)} configuration files")
            
            # Step 2: Submit MediaConvert jobs
            print("\nStep 2: Submitting MediaConvert jobs...")
            job_results = workflow.submit_mediaconvert_jobs(
                config_dir=args.output_dir,
                s3_source_path=args.s3_source_path,
                wait_for_completion=not args.no_wait,
                include_ids=include_ids,
                exclude_ids=exclude_ids
            )
            print(f"Submitted {len(job_results)} MediaConvert jobs")
            
            # If not waiting for completion, we can't proceed to analysis
            if args.no_wait:
                print("\nJobs submitted but not waiting for completion. Analysis step skipped.")
                return 0
            
            # Step 3: Analyze videos
            print("\nStep 3: Analyzing videos...")
            analysis_results = workflow.analyze_videos(
                s3_path=args.s3_source_path,
                include_ids=include_ids,
                exclude_ids=exclude_ids
            )
            
            print(f"Analyzed {len(analysis_results)} video pairs")
            for file_id, result in analysis_results.items():
                if 'error' in result:
                    print(f"ID {file_id}: Error - {result['error']}")
                else:
                    diff_status = "Has differences" if result['has_differences'] else "No differences"
                    print(f"ID {file_id}: {diff_status}")
                    print(f"  Result saved to: {result['result_path']}")
            
            return 0
    
    except Exception as e:
        print(f"Error: {str(e)}")
        return 1


if __name__ == '__main__':
    sys.exit(main())
