#!/usr/bin/env python3
"""
E2MC Workflow - Complete workflow for Encoding to MediaConvert conversion and comparison

This script provides a complete workflow for:
1. Converting Encoding.com configuration files to AWS MediaConvert configuration files
2. Creating MediaConvert transcoding jobs using the converted configurations
3. Analyzing and comparing the videos generated by both systems

Author: AWS Professional Services
"""

import argparse
import boto3
import json
import logging
import os
import re
import sys
import time
from datetime import datetime
import random
from typing import Dict, Any, List, Optional, Tuple
from pathlib import Path
from botocore.exceptions import ClientError

# Import required modules from the project
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../../../')))
from src.e2mc_assistant.converter.config_converter_enhanced import ConfigConverter
from src.e2mc_assistant.requester.mediaconvert_job_submitter import MediaConvertJobSubmitter
from src.e2mc_assistant.analyzer.video_analyzer import VideoAnalyzer

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class E2MCWorkflow:
    """
    Class to handle the complete workflow from Encoding.com configuration to MediaConvert
    job submission and video comparison.
    """

    # Constants for path handling
    S3_PREFIX = "s3://"

    def __init__(self, region: str = 'us-east-1', role_arn: Optional[str] = None):
        """
        Initialize the workflow handler.

        Args:
            region: AWS region for MediaConvert and S3 operations
            role_arn: IAM role ARN for MediaConvert to access resources
        """
        self.region = region
        self.role_arn = role_arn
        self.s3_client = boto3.client('s3', region_name=region)
        
        # Initialize components
        self.converter = None
        self.job_submitter = None
        self.video_analyzer = None

    def convert_configs(self, input_dir: str, output_dir: str, rules_file: str, template_file: Optional[str] = None, schema_file: Optional[str] = None) -> List[str]:
        """
        Convert Encoding.com configuration files to MediaConvert configuration files.

        Args:
            input_dir: Directory containing Encoding.com configuration files
            output_dir: Directory to save MediaConvert configuration files
            rules_file: Path to the mapping rules file
            template_file: Optional path to a template MediaConvert file
            schema_file: Optional path to a JSON schema file for validation

        Returns:
            List of paths to the generated MediaConvert configuration files
        """
        # Create output directory if it doesn't exist
        os.makedirs(output_dir, exist_ok=True)
        
        # Initialize converter
        self.converter = ConfigConverter(rules_file)
        
        # Configure logging for converter
        converter_logger = logging.getLogger('ConfigConverter')
        converter_logger.setLevel(logging.INFO)
        
        # Create a file handler for detailed logs
        log_file = os.path.join(output_dir, 'conversion_details.log')
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(logging.DEBUG)
        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
        converter_logger.addHandler(file_handler)
        
        # Track converted files
        converted_files = []
        
        # Process each XML file in the input directory
        for filename in os.listdir(input_dir):
            if filename.endswith('.xml'):
                source_file = os.path.join(input_dir, filename)
                
                # Extract ID from filename (assuming it's a number at the beginning)
                id_match = re.match(r'^(\d+)', filename)
                if id_match:
                    file_id = id_match.group(1)
                else:
                    file_id = os.path.splitext(filename)[0]
                
                # Define output filename with the same ID prefix
                output_file = os.path.join(output_dir, f"{file_id}.json")
                
                # Create a specific log file for this conversion
                file_log = os.path.join(output_dir, f"{file_id}_conversion.log")
                file_handler = logging.FileHandler(file_log)
                file_handler.setLevel(logging.DEBUG)
                file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
                converter_logger.addHandler(file_handler)
                
                try:
                    # Log the start of conversion
                    converter_logger.info(f"Starting conversion of {source_file}")
                    
                    # Convert the configuration
                    result = self.converter.convert(source_file, template_file)
                    
                    # Write the output file
                    with open(output_file, 'w') as f:
                        json.dump(result, f, indent=2)
                    
                    logger.info(f"Converted {source_file} to {output_file}")
                    converted_files.append(output_file)
                    
                    # Log successful conversion
                    converter_logger.info(f"Successfully converted {source_file} to {output_file}")
                    
                    # Validate the converted file if schema is provided
                    if schema_file:
                        from utils.mc_config_validator.validator import MediaConvertConfigValidator
                        validator = MediaConvertConfigValidator(schema_file)
                        converter_logger.info(f"Validating {output_file} against schema {schema_file}")
                        is_valid = validator.validate_config(output_file)
                        if not is_valid:
                            error_file = os.path.join(output_dir, f"{file_id}.err")
                            
                            # Create a string handler to capture validation errors
                            import io
                            string_io = io.StringIO()
                            string_handler = logging.StreamHandler(string_io)
                            string_handler.setLevel(logging.ERROR)
                            validator.logger.addHandler(string_handler)
                            
                            # Re-run validation to capture errors
                            validator.validate_config(output_file)
                            
                            # Get the captured error messages
                            validator.logger.removeHandler(string_handler)
                            error_messages = string_io.getvalue()
                            
                            # Write detailed error information to the error file
                            with open(error_file, 'w') as f:
                                f.write(f"Validation failed for {output_file}\n")
                                f.write("Validation errors:\n")
                                f.write(error_messages)
                            
                            converter_logger.error(f"Validation failed for {output_file}. Error log written to {error_file}")
                        else:
                            converter_logger.info(f"Validation successful for {output_file}")
                    
                except Exception as e:
                    logger.error(f"Error converting {source_file}: {str(e)}")
                    converter_logger.error(f"Error converting {source_file}: {str(e)}")
                
                # Remove the file-specific handler
                converter_logger.removeHandler(file_handler)
                file_handler.close()
        
        logger.info(f"Converted {len(converted_files)} configuration files")
        print(f"Detailed conversion logs saved to {log_file} and individual files in {output_dir}")
        return converted_files

    def submit_mediaconvert_jobs(self, config_dir: str, s3_source_path: str, wait_for_completion: bool = True) -> Dict[str, str]:
        """
        Submit MediaConvert jobs for each configuration file.

        Args:
            config_dir: Directory containing MediaConvert configuration files
            s3_source_path: S3 path where source videos are stored
            wait_for_completion: Whether to wait for each job to complete before submitting the next

        Returns:
            Dictionary mapping job IDs to their status
        """
        # Initialize job submitter
        self.job_submitter = MediaConvertJobSubmitter(
            region=self.region,
            role_arn=self.role_arn
        )
        
        # Configure logging for job submitter
        job_submitter_logger = logging.getLogger('MediaConvertJobSubmitter')
        job_submitter_logger.setLevel(logging.INFO)
        
        # Track job IDs and status
        job_results = {}
        
        # Process each JSON file in the config directory
        for filename in sorted(os.listdir(config_dir)):
            if filename.endswith('.json'):
                config_file = os.path.join(config_dir, filename)
                
                # Extract ID from filename (assuming it's a number at the beginning)
                id_match = re.match(r'^(\d+)', filename)
                if id_match:
                    file_id = id_match.group(1)
                else:
                    file_id = os.path.splitext(filename)[0]
                
                # Create a job log file in the same directory as the config file
                job_log_file = os.path.join(config_dir, f"{file_id}_job_submission.log")
                file_handler = logging.FileHandler(job_log_file)
                file_handler.setLevel(logging.INFO)
                file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
                job_submitter_logger.addHandler(file_handler)
                
                # Find source video in S3
                source_video = self._find_source_video(s3_source_path, file_id)
                if not source_video:
                    logger.warning(f"No source video found for ID {file_id}, skipping")
                    continue
                
                # Define output destination - normalize path to avoid double slashes
                s3_path_normalized = s3_source_path.rstrip('/')
                output_destination = f"{s3_path_normalized}/{file_id}/"
                
                try:
                    # Load job profile
                    job_profile = self.job_submitter.load_job_profile(config_file)
                    
                    # Update input URL and output destination
                    job_profile = self.job_submitter.update_input_url(job_profile, source_video)
                    job_profile = self.job_submitter.update_output_destination(job_profile, output_destination)
                    
                    # Submit the job
                    response = self.job_submitter.submit_job(job_profile)
                    job_id = response['Job']['Id']
                    logger.info(f"Submitted job for {file_id} with job ID: {job_id}")
                    job_submitter_logger.info(f"Submitted job for {file_id} with job ID: {job_id}")
                    job_submitter_logger.info(f"Job settings: {json.dumps(job_profile, indent=2)}")
                    
                    # Wait for job completion if requested
                    if wait_for_completion:
                        logger.info(f"Waiting for job {job_id} to complete...")
                        job_submitter_logger.info(f"Waiting for job {job_id} to complete...")
                        job = self.job_submitter.track_job(job_id)
                        job_results[job_id] = job['Status']
                        
                        # Log job completion status
                        job_submitter_logger.info(f"Job {job_id} completed with status: {job['Status']}")
                        try:
                            job_submitter_logger.info(f"Job details: {json.dumps(job, indent=2, default=str)}")
                        except Exception as json_err:
                            job_submitter_logger.error(f"Could not serialize job details: {str(json_err)}")
                        
                        if job['Status'] != MediaConvertJobSubmitter.STATUS_COMPLETE:
                            logger.warning(f"Job {job_id} completed with status: {job['Status']}")
                            job_submitter_logger.warning(f"Job {job_id} completed with status: {job['Status']}")
                            
                            # Create error log file for failed jobs
                            if job['Status'] == MediaConvertJobSubmitter.STATUS_ERROR:
                                error_file = os.path.join(config_dir, f"{file_id}_job_execution.err")
                                with open(error_file, 'w') as f:
                                    f.write(f"MediaConvert job {job_id} failed\n")
                                    f.write(f"Timestamp: {datetime.now().isoformat()}\n\n")
                                    f.write("Job details:\n")
                                    try:
                                        f.write(json.dumps(job, indent=2, default=str))
                                    except Exception as json_err:
                                        f.write(f"Could not serialize job details: {str(json_err)}")
                                    
                                    # Add error messages if available
                                    if 'ErrorMessage' in job:
                                        f.write("\n\nError message:\n")
                                        f.write(job['ErrorMessage'])
                                    elif 'ErrorCode' in job:
                                        f.write("\n\nError code:\n")
                                        f.write(job['ErrorCode'])
                                        
                                logger.error(f"Job {job_id} failed. Error details written to {error_file}")
                                job_submitter_logger.error(f"Job {job_id} failed. Error details written to {error_file}")
                    else:
                        job_results[job_id] = "SUBMITTED"
                    
                except Exception as e:
                    error_msg = f"Error submitting job for {file_id}: {str(e)}"
                    logger.error(error_msg)
                    job_submitter_logger.error(error_msg)
                    job_results[file_id] = f"ERROR: {str(e)}"
                    
                    # Create error log file for submission errors
                    error_file = os.path.join(config_dir, f"{file_id}_job_submission.err")
                    with open(error_file, 'w') as f:
                        f.write(f"Error submitting MediaConvert job for {file_id}\n")
                        f.write(f"Timestamp: {datetime.now().isoformat()}\n\n")
                        f.write(f"Error message: {str(e)}\n\n")
                        f.write("Job settings:\n")
                        try:
                            f.write(json.dumps(job_profile, indent=2, default=str))
                        except Exception as json_err:
                            f.write(f"Could not serialize job profile: {str(json_err)}")
                    
                    logger.error(f"Job submission error details written to {error_file}")
                    job_submitter_logger.error(f"Job submission error details written to {error_file}")
                
                # Remove the file-specific handler
                job_submitter_logger.removeHandler(file_handler)
                file_handler.close()
        
        return job_results

    def _find_source_video(self, s3_path: str, file_id: str) -> Optional[str]:
        """
        Find a source video in S3 with the given file ID prefix.

        Args:
            s3_path: S3 path where source videos are stored
            file_id: ID prefix to search for

        Returns:
            S3 URL of the source video if found, None otherwise
        """
        # Parse S3 path
        bucket_name, prefix = self._parse_s3_path(s3_path)
        
        # Create the prefix to search for
        search_prefix = f"{prefix}/{file_id}/" if prefix else f"{file_id}/"
        
        try:
            # List objects with the prefix
            response = self.s3_client.list_objects_v2(
                Bucket=bucket_name,
                Prefix=search_prefix
            )
            
            # Look for source video files
            for obj in response.get('Contents', []):
                key = obj['Key']
                if '_source.' in key.lower():
                    return f"s3://{bucket_name}/{key}"
            
            return None
        except Exception as e:
            logger.error(f"Error searching for source video: {str(e)}")
            return None

    def _parse_s3_path(self, s3_path: str) -> Tuple[str, str]:
        """
        Parse an S3 path into bucket name and prefix.

        Args:
            s3_path: S3 path (s3://bucket-name/prefix)

        Returns:
            Tuple of (bucket_name, prefix)
        """
        # Remove s3:// prefix if present
        if s3_path.startswith('s3://'):
            s3_path = s3_path[5:]
        
        # Split into bucket and prefix
        parts = s3_path.split('/', 1)
        bucket = parts[0]
        prefix = parts[1] if len(parts) > 1 else ""
        
        # Remove trailing slash from prefix
        if prefix and prefix.endswith('/'):
            prefix = prefix[:-1]
        
        return bucket, prefix

    def analyze_videos(self, s3_path: str) -> Dict[str, Any]:
        """
        Analyze and compare videos in the given S3 path.

        Args:
            s3_path: S3 path containing videos to analyze

        Returns:
            Dictionary containing analysis results
        """
        # Initialize video analyzer
        self.video_analyzer = VideoAnalyzer(region=self.region)
        
        # Parse S3 path
        bucket_name, prefix = self._parse_s3_path(s3_path)
        
        # Find all prefixes (directories) in the S3 path
        prefixes = self._list_s3_prefixes(bucket_name, prefix)
        
        # Track analysis results
        analysis_results = {}
        
        # Process each prefix (each should correspond to a file ID)
        for prefix_path in prefixes:
            # Extract ID from prefix
            prefix_parts = prefix_path.rstrip('/').split('/')
            file_id = prefix_parts[-1]
            
            # Create report directory structure
            normalized_prefix = prefix_path.rstrip('/')
            report_prefix = f"{normalized_prefix}/reports"
            
            # Find target videos (original and MediaConvert)
            original_video = self._find_original_video(bucket_name, prefix_path)
            mc_video = self._find_mc_video(bucket_name, prefix_path)
            
            if not original_video or not mc_video:
                logger.warning(f"Missing videos for ID {file_id}, skipping")
                continue
            
            try:
                # Extract video information
                logger.info(f"Analyzing videos for ID {file_id}")
                original_info = self.video_analyzer.extract_video_info(original_video)
                mc_info = self.video_analyzer.extract_video_info(mc_video)
                
                # Generate comprehensive report
                report = self.video_analyzer.generate_report(
                    original_info=original_info,
                    mc_info=mc_info,
                    s3_client=self.s3_client,
                    bucket_name=bucket_name,
                    report_prefix=report_prefix
                )
                
                # Add to results
                analysis_results[file_id] = {
                    'original_video': original_video,
                    'mc_video': mc_video,
                    'has_differences': bool(report.get('differences')),
                    'report_paths': report.get('report_paths', {})
                }
                
                logger.info(f"Completed analysis for ID {file_id}")
                
            except Exception as e:
                logger.error(f"Error analyzing videos for ID {file_id}: {str(e)}")
                analysis_results[file_id] = {
                    'original_video': original_video,
                    'mc_video': mc_video,
                    'error': str(e)
                }
        
        return analysis_results

    def _list_s3_prefixes(self, bucket_name: str, prefix: str) -> List[str]:
        """
        List all prefixes (directories) in the given S3 path.

        Args:
            bucket_name: S3 bucket name
            prefix: S3 prefix to list

        Returns:
            List of prefixes
        """
        prefixes = []
        
        # Ensure prefix ends with a slash
        if prefix and not prefix.endswith('/'):
            prefix = f"{prefix}/"
        
        try:
            # Use delimiter to list "directories"
            paginator = self.s3_client.get_paginator('list_objects_v2')
            for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix, Delimiter='/'):
                for prefix_obj in page.get('CommonPrefixes', []):
                    prefixes.append(prefix_obj['Prefix'])
        except Exception as e:
            logger.error(f"Error listing S3 prefixes: {str(e)}")
        
        return prefixes



    def _find_original_video(self, bucket_name: str, prefix: str) -> Optional[str]:
        """
        Find the original target video in S3 with pattern {id}_{可忽略的字符串}_target.{suffix}

        Args:
            bucket_name: S3 bucket name
            prefix: S3 prefix to search in

        Returns:
            S3 URL of the video if found, None otherwise
        """
        try:
            # List objects with the prefix
            response = self.s3_client.list_objects_v2(
                Bucket=bucket_name,
                Prefix=prefix
            )
            
            # Extract ID from prefix for matching
            prefix_parts = prefix.rstrip('/').split('/')
            file_id = prefix_parts[-1]
            
            # Look for files matching the pattern {id}_{可忽略的字符串}_target.{suffix}
            import re
            pattern = re.compile(f"^{re.escape(prefix)}{re.escape(file_id)}_.*_target\\.[a-zA-Z0-9]+$", re.IGNORECASE)
            
            for obj in response.get('Contents', []):
                key = obj['Key']
                if pattern.match(key):
                    logger.debug(f"Found original video: {key}")
                    return f"s3://{bucket_name}/{key}"
            
            # Fallback to simpler pattern if regex didn't match
            for obj in response.get('Contents', []):
                key = obj['Key']
                if "_target." in key.lower() and not "_mc." in key.lower():
                    logger.debug(f"Found original video (fallback): {key}")
                    return f"s3://{bucket_name}/{key}"
            
            logger.warning(f"No original video found in {prefix}")
            return None
        except Exception as e:
            logger.error(f"Error searching for original video: {str(e)}")
            return None
            
    def _find_mc_video(self, bucket_name: str, prefix: str) -> Optional[str]:
        """
        Find the MediaConvert video in S3 with pattern {id}_{可忽略的字符串}_source_{可忽略的字符串}_{可忽略的字符串}_mc.{suffix}

        Args:
            bucket_name: S3 bucket name
            prefix: S3 prefix to search in

        Returns:
            S3 URL of the video if found, None otherwise
        """
        try:
            # List objects with the prefix
            response = self.s3_client.list_objects_v2(
                Bucket=bucket_name,
                Prefix=prefix
            )
            
            # Extract ID from prefix for matching
            prefix_parts = prefix.rstrip('/').split('/')
            file_id = prefix_parts[-1]
            
            # Look for files matching the pattern {id}_{可忽略的字符串}_source_{可忽略的字符串}_{可忽略的字符串}_mc.{suffix}
            import re
            pattern = re.compile(f"^{re.escape(prefix)}{re.escape(file_id)}_.*_source_.*_.*_mc\\.[a-zA-Z0-9]+$", re.IGNORECASE)
            
            for obj in response.get('Contents', []):
                key = obj['Key']
                if pattern.match(key):
                    logger.debug(f"Found MC video: {key}")
                    return f"s3://{bucket_name}/{key}"
            
            # Fallback to simpler pattern if regex didn't match
            for obj in response.get('Contents', []):
                key = obj['Key']
                if "_source_" in key.lower() and "_mc." in key.lower():
                    logger.debug(f"Found MC video (fallback): {key}")
                    return f"s3://{bucket_name}/{key}"
            
            logger.warning(f"No MediaConvert video found in {prefix}")
            return None
        except Exception as e:
            logger.error(f"Error searching for MediaConvert video: {str(e)}")
            return None




    def _save_to_s3(self, bucket_name: str, key: str, content: str) -> bool:
        """
        Save content to an S3 object.

        Args:
            bucket_name: S3 bucket name
            key: S3 object key
            content: Content to save

        Returns:
            True if successful, False otherwise
        """
        try:
            self.s3_client.put_object(
                Bucket=bucket_name,
                Key=key,
                Body=content
            )
            return True
        except Exception as e:
            logger.error(f"Error saving to S3: {str(e)}")
            return False


def parse_arguments():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description='E2MC Workflow - Complete workflow for Encoding to MediaConvert conversion and comparison'
    )
    
    # Create subparsers for different commands
    subparsers = parser.add_subparsers(dest='command', help='Command to execute')
    
    # Convert command
    convert_parser = subparsers.add_parser(
        'convert',
        help='Convert Encoding.com configuration files to MediaConvert configuration files'
    )
    convert_parser.add_argument(
        '--input-dir',
        required=True,
        help='Directory containing Encoding.com configuration files'
    )
    convert_parser.add_argument(
        '--output-dir',
        required=True,
        help='Directory to save MediaConvert configuration files'
    )
    convert_parser.add_argument(
        '--rules-file',
        required=True,
        help='Path to the mapping rules file'
    )
    convert_parser.add_argument(
        '--template-file',
        help='Optional path to a template MediaConvert file'
    )
    convert_parser.add_argument(
        '--region',
        default='us-east-1',
        help='AWS region (default: us-east-1)'
    )
    convert_parser.add_argument(
        '--validate',
        help='Path to JSON schema file for validation'
    )
    
    # Submit command
    submit_parser = subparsers.add_parser(
        'submit',
        help='Submit MediaConvert jobs for configuration files'
    )
    submit_parser.add_argument(
        '--config-dir',
        required=True,
        help='Directory containing MediaConvert configuration files'
    )
    submit_parser.add_argument(
        '--s3-source-path',
        required=True,
        help='S3 path where source videos are stored (s3://bucket-name/prefix)'
    )
    submit_parser.add_argument(
        '--region',
        default='us-east-1',
        help='AWS region (default: us-east-1)'
    )
    submit_parser.add_argument(
        '--role-arn',
        help='IAM role ARN for MediaConvert to access resources'
    )
    submit_parser.add_argument(
        '--no-wait',
        action='store_true',
        help='Do not wait for jobs to complete before submitting the next'
    )
    
    # Analyze command
    analyze_parser = subparsers.add_parser(
        'analyze',
        help='Analyze and compare videos'
    )
    analyze_parser.add_argument(
        '--s3-path',
        required=True,
        help='S3 path containing videos to analyze (s3://bucket-name/prefix)'
    )
    analyze_parser.add_argument(
        '--region',
        default='us-east-1',
        help='AWS region (default: us-east-1)'
    )
    analyze_parser.add_argument(
        '--use-llm',
        action='store_true',
        help='Use LLM (Amazon Bedrock) to analyze video differences'
    )
    
    # Full workflow command
    workflow_parser = subparsers.add_parser(
        'workflow',
        help='Run the complete workflow (convert, submit, analyze)'
    )
    workflow_parser.add_argument(
        '--input-dir',
        required=True,
        help='Directory containing Encoding.com configuration files'
    )
    workflow_parser.add_argument(
        '--output-dir',
        required=True,
        help='Directory to save MediaConvert configuration files'
    )
    workflow_parser.add_argument(
        '--rules-file',
        required=True,
        help='Path to the mapping rules file'
    )
    workflow_parser.add_argument(
        '--template-file',
        help='Optional path to a template MediaConvert file'
    )
    workflow_parser.add_argument(
        '--s3-source-path',
        required=True,
        help='S3 path where source videos are stored (s3://bucket-name/prefix)'
    )
    workflow_parser.add_argument(
        '--region',
        default='us-east-1',
        help='AWS region (default: us-east-1)'
    )
    workflow_parser.add_argument(
        '--role-arn',
        help='IAM role ARN for MediaConvert to access resources'
    )
    workflow_parser.add_argument(
        '--no-wait',
        action='store_true',
        help='Do not wait for jobs to complete before submitting the next'
    )
    
    return parser.parse_args()


def main():
    """Main function."""
    args = parse_arguments()
    
    if not args.command:
        print("No command specified. Use --help for usage information.")
        return 1
    
    try:
        # Initialize workflow handler
        workflow = E2MCWorkflow(
            region=args.region,
            role_arn=getattr(args, 'role_arn', None)
        )
        
        if args.command == 'convert':
            # Convert configuration files
            converted_files = workflow.convert_configs(
                input_dir=args.input_dir,
                output_dir=args.output_dir,
                rules_file=args.rules_file,
                template_file=args.template_file,
                schema_file=args.validate if hasattr(args, 'validate') else None
            )
            
            print(f"Converted {len(converted_files)} configuration files")
            return 0
            
        elif args.command == 'submit':
            # Submit MediaConvert jobs
            job_results = workflow.submit_mediaconvert_jobs(
                config_dir=args.config_dir,
                s3_source_path=args.s3_source_path,
                wait_for_completion=not args.no_wait
            )
            
            print(f"Submitted {len(job_results)} MediaConvert jobs")
            for job_id, status in job_results.items():
                print(f"Job {job_id}: {status}")
            
            return 0
            
        elif args.command == 'analyze':
            # Analyze videos
            analysis_results = workflow.analyze_videos(
                s3_path=args.s3_path
            )
            
            print(f"Analyzed {len(analysis_results)} video pairs")
            for file_id, result in analysis_results.items():
                if 'error' in result:
                    print(f"ID {file_id}: Error - {result['error']}")
                else:
                    diff_status = "Has differences" if result['has_differences'] else "No differences"
                    print(f"ID {file_id}: {diff_status}")
                    print(f"  Reports saved to:")
                    for report_type, path in result['report_paths'].items():
                        if path:
                            print(f"    - {report_type}: {path}")
            
            return 0
            
        elif args.command == 'workflow':
            # Run the complete workflow
            
            # Step 1: Convert configuration files
            print("Step 1: Converting configuration files...")
            converted_files = workflow.convert_configs(
                input_dir=args.input_dir,
                output_dir=args.output_dir,
                rules_file=args.rules_file,
                template_file=args.template_file
            )
            print(f"Converted {len(converted_files)} configuration files")
            
            # Step 2: Submit MediaConvert jobs
            print("\nStep 2: Submitting MediaConvert jobs...")
            job_results = workflow.submit_mediaconvert_jobs(
                config_dir=args.output_dir,
                s3_source_path=args.s3_source_path,
                wait_for_completion=not args.no_wait
            )
            print(f"Submitted {len(job_results)} MediaConvert jobs")
            
            # If not waiting for completion, we can't proceed to analysis
            if args.no_wait:
                print("\nJobs submitted but not waiting for completion. Analysis step skipped.")
                return 0
            
            # Step 3: Analyze videos
            print("\nStep 3: Analyzing videos...")
            analysis_results = workflow.analyze_videos(
                s3_path=args.s3_source_path
            )
            
            print(f"Analyzed {len(analysis_results)} video pairs")
            for file_id, result in analysis_results.items():
                if 'error' in result:
                    print(f"ID {file_id}: Error - {result['error']}")
                else:
                    diff_status = "Has differences" if result['has_differences'] else "No differences"
                    print(f"ID {file_id}: {diff_status}")
                    print(f"  Result saved to: {result['result_path']}")
            
            return 0
    
    except Exception as e:
        print(f"Error: {str(e)}")
        return 1


if __name__ == '__main__':
    sys.exit(main())
